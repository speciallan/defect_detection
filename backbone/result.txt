1、使用默认lr,使用1：8正负样本数据

最早训练acc直接0.99，但是验证集精度和损失此起彼伏。
loss和valloss差距很大。
训练loss波动。

分析：学习率的问题，或者模型问题。迁移学习，lr过大会破坏预训练权重。

2、修改为学习率自适应减少
到20epoch左右，val_acc开始稳定增加，直到40epoch后稳定。

分析：能找到合适lr，解决1的问题。但是一定epoch后acc不提高，继续调整学习率。


3、后面初始lr就设置为0.001
出现acc 0.999，loss 0.001
但是val_acc只有0.05且不增加，val_loss很大的情况。

分析：学习率过小，造成网络学习极慢。

4、初始lr=0.01 + lr自适应减少
lr=0.01, decay=1e-6, momentum=0.9
此时acc和val_acc都能到0.999，loss在0.01左右，val_loss在0.5左右


分析：此时学习率合理，模型精度还不错。但是验证损失还是有一点点大，应该是数据集的问题。

此时通过evaluate.py进行评估(从测试集取一部分，200条数据进行)，发现分类正反例概率相近，精度很低。模型泛化能力极差。

分析：可能是数据集太小，对模型训练不够，还有没对数据做预处理。


5、加入数据增强(一次性增强）全样本

所有数值均正常，acc稳定增加，loss稳定减少，30个epoch训练完成。
loss稳定在0.0001，val_loss在0.062
acc达到1.000 val_acc达到0.98以上

test_acc达到0.985

6、使用80%样本进行训练

loss稳定在0.0001，val_loss在0.062
acc达到1.000 val_acc达到0.98以上

但是test_acc只有0.9左右，说明模型泛化能力不够。

分析：划分测试集验证集的时候没有打乱，造成数据分布不均衡。


7、按8:2随机划分

使用train_test_split

果然使用了随机划分后，就不会受到某几个difficult sample的影响，val_acc回到0.985

test_acc也达到了0.995甚至1.000


8、使用生成器做数据增强（每个batch_size增强）

在修改为随机划分后，这里的acc和val_acc都能到0.991(比上面的0.985还高），毕竟使用了数据增强

test_acc也在0.99-0.995-1.000左右，泛化性能不错。


9、调整微调层数

使用上面训练的resnet50训练rpn，网络精度很低。分析原因是，训练的时候是resnet50，rpn的特征提取器使用的前40层，没有匹配好。

所以这里我先训练resnet40，得到test_acc为0.85左右，明显降低。再去训练rpn


10、数据去均值处理


